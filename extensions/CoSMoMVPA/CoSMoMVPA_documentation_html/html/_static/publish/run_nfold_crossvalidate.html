
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>n-fold cross-validation classification with LDA classifier</title><meta name="generator" content="MATLAB 8.5"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2016-12-01"><meta name="DC.source" content="run_nfold_crossvalidate.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>n-fold cross-validation classification with LDA classifier</h1><!--introduction--><div><ol><li>For CoSMoMVPA's copyright information and license terms,   #</li><li>see the COPYING file distributed with CoSMoMVPA.           #</li></ol></div><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Define data</a></li><li><a href="#2">set sample attributes</a></li><li><a href="#3">Part 1: 'manual' crossvalidation</a></li><li><a href="#4">Part 2: use cosmo_nfold_partitioner</a></li></ul></div><h2>Define data<a name="1"></a></h2><pre class="codeinput">config=cosmo_config();
data_path=fullfile(config.tutorial_data_path,<span class="string">'ak6'</span>,<span class="string">'s01'</span>);

<span class="comment">% Load the dataset with VT mask</span>
ds = cosmo_fmri_dataset([data_path <span class="string">'/glm_T_stats_perrun.nii'</span>], <span class="keyword">...</span>
                     <span class="string">'mask'</span>, [data_path <span class="string">'/vt_mask.nii'</span>]);

<span class="comment">% remove constant features</span>
ds=cosmo_remove_useless_data(ds);
</pre><h2>set sample attributes<a name="2"></a></h2><pre class="codeinput">ds.sa.targets = repmat((1:6)',10,1);
ds.sa.chunks = floor(((1:60)-1)/6)'+1;

<span class="comment">% Add labels as sample attributes</span>
classes = {<span class="string">'monkey'</span>,<span class="string">'lemur'</span>,<span class="string">'mallard'</span>,<span class="string">'warbler'</span>,<span class="string">'ladybug'</span>,<span class="string">'lunamoth'</span>};
ds.sa.labels = repmat(classes,1,10)';

<span class="comment">% this is good practice after setting attributes manually</span>
cosmo_check_dataset(ds);
</pre><h2>Part 1: 'manual' crossvalidation<a name="3"></a></h2><pre class="codeinput">nsamples=size(ds.samples,1); <span class="comment">% should be 60 for this dataset</span>

<span class="comment">% allocate space for preditions for all 60 samples</span>
all_pred=zeros(nsamples,1);

<span class="comment">% safety check:</span>
<span class="comment">% to simplify this exercise, the code below assumes that .sa.chunks is in</span>
<span class="comment">% the range 1..10; if that is not the case, the code may not work properly.</span>
<span class="comment">% Therefore an 'assert' statement is used to verify that the chunks are as</span>
<span class="comment">% required for the remainder of this exercise.</span>
assert(isequal(ds.sa.chunks,floor(((1:60)-1)/6)'+1));

nfolds=numel(unique(ds.sa.chunks)); <span class="comment">% should be 10</span>

<span class="comment">% run n-fold cross-validation</span>
<span class="comment">% in the k-th fold (k ranges from 1 to 10), test the LDA classifier on</span>
<span class="comment">% samples with chunks==k and after training on all other samples.</span>
<span class="comment">%</span>
<span class="comment">% (in this exercise this is done manually, but easier solutions involve</span>
<span class="comment">% using cosmo_nfold_partitioner and cosmo_crossvalidation_measure)</span>
<span class="keyword">for</span> fold=1:nfolds
    <span class="comment">% make a logical mask (of size 60x1) for the test set. It should have</span>
    <span class="comment">% the value true where ds.sa.chunks has the same value as 'fold', and</span>
    <span class="comment">% the value false everywhere else. Assign this to the variable</span>
    <span class="comment">% 'test_msk'</span>
    <span class="comment">% &gt;@@&gt;</span>
    test_msk=ds.sa.chunks==fold;
    <span class="comment">% &lt;@@&lt;</span>

    <span class="comment">% slice the input dataset 'ds' across samples using 'test_msk' so that</span>
    <span class="comment">% it has only samples in the 'fold'-th chunk. Assign the result to the</span>
    <span class="comment">% variable 'ds_test';</span>
    <span class="comment">% &gt;@@&gt;</span>
    ds_test=cosmo_slice(ds,test_msk);
    <span class="comment">% &lt;@@&lt;</span>

    <span class="comment">% now make another logical mask (of size 60x1) for the training set.</span>
    <span class="comment">% the value true where ds.sa.chunks has a different value as 'fold',</span>
    <span class="comment">% and the value false everywhere else. Assign this to the variable</span>
    <span class="comment">% 'train_msk'</span>
    <span class="comment">% &gt;@@&gt;</span>
    train_msk=ds.sa.chunks~=fold;
    <span class="comment">% (alternative: train_msk=~test_msk)</span>
    <span class="comment">% &lt;@@&lt;</span>

    <span class="comment">% slice the input dataset again using train_msk, and assign to the</span>
    <span class="comment">% variable 'ds_train'</span>
    <span class="comment">% &gt;@@&gt;</span>
    ds_train=cosmo_slice(ds,train_msk);
    <span class="comment">% &lt;@@&lt;</span>

    <span class="comment">% Use cosmo_classify_lda to get predicted targets for the</span>
    <span class="comment">% samples in 'ds_test'. To do so, use the samples and targets</span>
    <span class="comment">% from 'ds_train' for training (as first and second argument for</span>
    <span class="comment">% cosmo_classify_lda), and the samples from 'ds_test' for testing</span>
    <span class="comment">% (third argument for cosmo_classify_lda).</span>
    <span class="comment">% Assign the result to the variable 'fold_pred', which should be a 6x1</span>
    <span class="comment">% vector.</span>
    <span class="comment">% &gt;@@&gt;</span>
    fold_pred=cosmo_classify_lda(ds_train.samples,ds_train.sa.targets,<span class="keyword">...</span>
                                    ds_test.samples);
    <span class="comment">% &lt;@@&lt;</span>

    <span class="comment">% store the predictions from 'fold_pred' in the 'all_pred' vector,</span>
    <span class="comment">% at the positions masked by 'test_msk'.</span>
    <span class="comment">% &gt;@@&gt;</span>
    all_pred(test_msk)=fold_pred;
    <span class="comment">% &lt;@@&lt;</span>
<span class="keyword">end</span>

<span class="comment">% safety check:</span>
<span class="comment">% for this exercise, the following code tests whether the predicted classes</span>
<span class="comment">% is as they should be (i.e. the correct answer); if not an error is</span>
<span class="comment">% raised.</span>

expected_pred=[ 1 1 1 2 1 1 2 1 1 1
                2 1 2 2 2 2 2 2 2 2
                4 3 1 3 3 4 3 3 3 3
                4 4 2 4 4 4 4 4 3 2
                5 5 5 5 5 6 5 5 5 5
                6 6 6 6 6 6 6 6 6 6];

<span class="comment">% check that the output is as expected</span>
<span class="keyword">if</span> ~isequal(expected_pred(:),all_pred)
    error(<span class="string">'expected predictions to be row-vector with [%s]'''</span>,<span class="keyword">...</span>
            sprintf(<span class="string">'%d '</span>,all_pred_alt));
<span class="keyword">end</span>

<span class="comment">% Compute classification accuracy of all_pred compared to the targets in</span>
<span class="comment">% the input dataset 'ds', and assign to a variable 'accuracy'</span>
<span class="comment">% &gt;@@&gt;</span>
accuracy=mean(all_pred==ds.sa.targets);
<span class="comment">% &lt;@@&lt;</span>

<span class="comment">% print the accuracy</span>
fprintf(<span class="string">'\nLDA all categories n-fold: accuracy %.3f\n'</span>, accuracy);

<span class="comment">% Visualize confusion matrix</span>
<span class="comment">% the cosmo_confusion_matrix convenience function is used to compute the</span>
<span class="comment">% confusion matrix</span>
[confusion_matrix,classes]=cosmo_confusion_matrix(ds.sa.targets,all_pred);
nclasses=numel(classes);
<span class="comment">% print confusion matrix to terminal window</span>
fprintf(<span class="string">'\nLDA n-fold cross-validation confusion matrix:\n'</span>)
disp(confusion_matrix);

<span class="comment">% make a pretty figure</span>
figure
imagesc(confusion_matrix,[0 10])
title(<span class="string">'confusion matrix'</span>);
set(gca,<span class="string">'XTick'</span>,1:nclasses,<span class="string">'XTickLabel'</span>,classes);
set(gca,<span class="string">'YTick'</span>,1:nclasses,<span class="string">'YTickLabel'</span>,classes);
ylabel(<span class="string">'target'</span>);
xlabel(<span class="string">'predicted'</span>);
colorbar
</pre><pre class="codeoutput">
LDA all categories n-fold: accuracy 0.833

LDA n-fold cross-validation confusion matrix:
     8     2     0     0     0     0
     1     9     0     0     0     0
     1     0     7     2     0     0
     0     2     1     7     0     0
     0     0     0     0     9     1
     0     0     0     0     0    10

</pre><img vspace="5" hspace="5" src="run_nfold_crossvalidate_01.png" alt=""> <h2>Part 2: use cosmo_nfold_partitioner<a name="4"></a></h2><pre class="codeinput"><span class="comment">% This exercise replicates the analysis done in Part 1, but now</span>
<span class="comment">% the the 'cosmo_nfold_partitioner' function is used to create a struct</span>
<span class="comment">% that defines the cross-validation scheme (it contains the indices</span>
<span class="comment">% for the train and set samples in each fold)</span>
partitions=cosmo_nfold_partitioner(ds);

<span class="comment">% Show partitions</span>
fprintf(<span class="string">'\nPartitions for n-fold cross-validation:\n'</span>);
cosmo_disp(partitions)

<span class="comment">% Count how many folds there are in 'partitions', and assign to the</span>
<span class="comment">% variable 'nfolds'</span>
<span class="comment">% &gt;@@&gt;</span>
nfolds=numel(partitions.train_indices); <span class="comment">% should be 10</span>
<span class="comment">% &lt;@@&lt;</span>

<span class="comment">% allocate space for predictions of each sample (pattern) in 'ds'</span>
all_pred=zeros(nsamples,1);

<span class="comment">% As in part 1 (above), perform n-fold cross-validation using the LDA</span>
<span class="comment">% classifier</span>
<span class="keyword">for</span> fold=1:nfolds
    <span class="comment">% implement cross-validation and store predicted labels in 'all_pred';</span>
    <span class="comment">% use the contents of partitions to slice the dataset in train and test</span>
    <span class="comment">% sets for each fold</span>

    <span class="comment">% from the 'partitions' struct, get the train indices for the</span>
    <span class="comment">% 'fold'-th fold and assign to a variable 'train_idxs'</span>
    <span class="comment">% &gt;@@&gt;</span>
    train_idxs=partitions.train_indices{fold};
    <span class="comment">% &lt;@@&lt;</span>

    <span class="comment">% do the same for the test indices, and assign to a variable</span>
    <span class="comment">% 'test_idxs'</span>
    <span class="comment">% &gt;@@&gt;</span>
    test_idxs=partitions.test_indices{fold};
    <span class="comment">% &lt;@@&lt;</span>

    <span class="comment">% slice the dataset twice:</span>
    <span class="comment">% - once using 'train_idxs'; assign the result to 'ds_train'</span>
    <span class="comment">% - once using 'test_idxs' ; assign the result to 'ds_test'</span>
    <span class="comment">% &gt;@@&gt;</span>
    ds_train=cosmo_slice(ds,train_idxs);
    ds_test=cosmo_slice(ds,test_idxs);
    <span class="comment">% &lt;@@&lt;</span>

    <span class="comment">% compute predictions for the samples in 'ds_test' after training</span>
    <span class="comment">% using the samples and targets in 'ds_train'</span>
    <span class="comment">% &gt;@@&gt;</span>
    fold_pred=cosmo_classify_lda(ds_train.samples,ds_train.sa.targets,<span class="keyword">...</span>
                                    ds_test.samples);
    <span class="comment">% &lt;@@&lt;</span>

     <span class="comment">% store the predictions from 'fold_pred' in the 'all_pred' vector,</span>
    <span class="comment">% at the positions indexed by 'test_idxs'.</span>
    <span class="comment">% &gt;@@&gt;</span>
    all_pred(test_idxs)=fold_pred;
    <span class="comment">% &lt;@@&lt;</span>
<span class="keyword">end</span>

<span class="comment">% Compute classification accuracy of all_pred compared to the targets in</span>
<span class="comment">% the input dataset 'ds', and assign to a variable 'accuracy'</span>
<span class="comment">% &gt;@@&gt;</span>
accuracy=mean(all_pred==ds.sa.targets);
<span class="comment">% &lt;@@&lt;</span>
fprintf([<span class="string">'\nLDA all categories n-fold (with partitioner): '</span><span class="keyword">...</span>
            <span class="string">'accuracy %.3f\n'</span>], accuracy);

<span class="comment">% Note: cosmo_crossvalidation_measure can perform the above operations as</span>
<span class="comment">% well (and in an easier way), but using that function is not part of</span>
<span class="comment">% this exercise.</span>
</pre><pre class="codeoutput">
Partitions for n-fold cross-validation:
.train_indices                                                                              
  { [  7         [  1         [  1        ... [  1         [  1         [  1                
       8            2            2               2            2            2                
       9            3            3               3            3            3                
       :            :            :               :            :            :                
      58           58           58              58           58           52                
      59           59           59              59           59           53                
      60 ]@54x1    60 ]@54x1    60 ]@54x1       60 ]@54x1    60 ]@54x1    54 ]@54x1   }@1x10
.test_indices                                                                               
  { [ 1    [  7    [ 13   ... [ 43    [ 49    [ 55                                          
      2       8      14         44      50      56                                          
      3       9      15         45      51      57                                          
      4      10      16         46      52      58                                          
      5      11      17         47      53      59                                          
      6 ]    12 ]    18 ]       48 ]    54 ]    60 ]   }@1x10                               

LDA all categories n-fold (with partitioner): accuracy 0.833
</pre><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2015a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% n-fold cross-validation classification with LDA classifier
%
% #   For CoSMoMVPA's copyright information and license terms,   #
% #   see the COPYING file distributed with CoSMoMVPA.           #

%% Define data
config=cosmo_config();
data_path=fullfile(config.tutorial_data_path,'ak6','s01');

% Load the dataset with VT mask
ds = cosmo_fmri_dataset([data_path '/glm_T_stats_perrun.nii'], ...
                     'mask', [data_path '/vt_mask.nii']);

% remove constant features
ds=cosmo_remove_useless_data(ds);

%% set sample attributes

ds.sa.targets = repmat((1:6)',10,1);
ds.sa.chunks = floor(((1:60)-1)/6)'+1;

% Add labels as sample attributes
classes = {'monkey','lemur','mallard','warbler','ladybug','lunamoth'};
ds.sa.labels = repmat(classes,1,10)';

% this is good practice after setting attributes manually
cosmo_check_dataset(ds);

%% Part 1: 'manual' crossvalidation

nsamples=size(ds.samples,1); % should be 60 for this dataset

% allocate space for preditions for all 60 samples
all_pred=zeros(nsamples,1);

% safety check:
% to simplify this exercise, the code below assumes that .sa.chunks is in
% the range 1..10; if that is not the case, the code may not work properly.
% Therefore an 'assert' statement is used to verify that the chunks are as
% required for the remainder of this exercise.
assert(isequal(ds.sa.chunks,floor(((1:60)-1)/6)'+1));

nfolds=numel(unique(ds.sa.chunks)); % should be 10

% run n-fold cross-validation
% in the k-th fold (k ranges from 1 to 10), test the LDA classifier on
% samples with chunks==k and after training on all other samples.
%
% (in this exercise this is done manually, but easier solutions involve
% using cosmo_nfold_partitioner and cosmo_crossvalidation_measure)
for fold=1:nfolds
    % make a logical mask (of size 60x1) for the test set. It should have
    % the value true where ds.sa.chunks has the same value as 'fold', and
    % the value false everywhere else. Assign this to the variable
    % 'test_msk'
    % >@@>
    test_msk=ds.sa.chunks==fold;
    % <@@<

    % slice the input dataset 'ds' across samples using 'test_msk' so that
    % it has only samples in the 'fold'-th chunk. Assign the result to the
    % variable 'ds_test';
    % >@@>
    ds_test=cosmo_slice(ds,test_msk);
    % <@@<

    % now make another logical mask (of size 60x1) for the training set.
    % the value true where ds.sa.chunks has a different value as 'fold',
    % and the value false everywhere else. Assign this to the variable
    % 'train_msk'
    % >@@>
    train_msk=ds.sa.chunks~=fold;
    % (alternative: train_msk=~test_msk)
    % <@@<

    % slice the input dataset again using train_msk, and assign to the
    % variable 'ds_train'
    % >@@>
    ds_train=cosmo_slice(ds,train_msk);
    % <@@<

    % Use cosmo_classify_lda to get predicted targets for the
    % samples in 'ds_test'. To do so, use the samples and targets
    % from 'ds_train' for training (as first and second argument for
    % cosmo_classify_lda), and the samples from 'ds_test' for testing
    % (third argument for cosmo_classify_lda).
    % Assign the result to the variable 'fold_pred', which should be a 6x1
    % vector.
    % >@@>
    fold_pred=cosmo_classify_lda(ds_train.samples,ds_train.sa.targets,...
                                    ds_test.samples);
    % <@@<

    % store the predictions from 'fold_pred' in the 'all_pred' vector,
    % at the positions masked by 'test_msk'.
    % >@@>
    all_pred(test_msk)=fold_pred;
    % <@@<
end

% safety check:
% for this exercise, the following code tests whether the predicted classes
% is as they should be (i.e. the correct answer); if not an error is
% raised.

expected_pred=[ 1 1 1 2 1 1 2 1 1 1
                2 1 2 2 2 2 2 2 2 2
                4 3 1 3 3 4 3 3 3 3
                4 4 2 4 4 4 4 4 3 2
                5 5 5 5 5 6 5 5 5 5
                6 6 6 6 6 6 6 6 6 6];

% check that the output is as expected
if ~isequal(expected_pred(:),all_pred)
    error('expected predictions to be row-vector with [%s]''',...
            sprintf('%d ',all_pred_alt));
end

% Compute classification accuracy of all_pred compared to the targets in
% the input dataset 'ds', and assign to a variable 'accuracy'
% >@@>
accuracy=mean(all_pred==ds.sa.targets);
% <@@<

% print the accuracy
fprintf('\nLDA all categories n-fold: accuracy %.3f\n', accuracy);

% Visualize confusion matrix
% the cosmo_confusion_matrix convenience function is used to compute the
% confusion matrix
[confusion_matrix,classes]=cosmo_confusion_matrix(ds.sa.targets,all_pred);
nclasses=numel(classes);
% print confusion matrix to terminal window
fprintf('\nLDA n-fold cross-validation confusion matrix:\n')
disp(confusion_matrix);

% make a pretty figure
figure
imagesc(confusion_matrix,[0 10])
title('confusion matrix');
set(gca,'XTick',1:nclasses,'XTickLabel',classes);
set(gca,'YTick',1:nclasses,'YTickLabel',classes);
ylabel('target');
xlabel('predicted');
colorbar

%% Part 2: use cosmo_nfold_partitioner

% This exercise replicates the analysis done in Part 1, but now
% the the 'cosmo_nfold_partitioner' function is used to create a struct
% that defines the cross-validation scheme (it contains the indices
% for the train and set samples in each fold)
partitions=cosmo_nfold_partitioner(ds);

% Show partitions
fprintf('\nPartitions for n-fold cross-validation:\n');
cosmo_disp(partitions)

% Count how many folds there are in 'partitions', and assign to the
% variable 'nfolds'
% >@@>
nfolds=numel(partitions.train_indices); % should be 10
% <@@<

% allocate space for predictions of each sample (pattern) in 'ds'
all_pred=zeros(nsamples,1);

% As in part 1 (above), perform n-fold cross-validation using the LDA
% classifier
for fold=1:nfolds
    % implement cross-validation and store predicted labels in 'all_pred';
    % use the contents of partitions to slice the dataset in train and test
    % sets for each fold

    % from the 'partitions' struct, get the train indices for the
    % 'fold'-th fold and assign to a variable 'train_idxs'
    % >@@>
    train_idxs=partitions.train_indices{fold};
    % <@@<

    % do the same for the test indices, and assign to a variable
    % 'test_idxs'
    % >@@>
    test_idxs=partitions.test_indices{fold};
    % <@@<

    % slice the dataset twice:
    % - once using 'train_idxs'; assign the result to 'ds_train'
    % - once using 'test_idxs' ; assign the result to 'ds_test'
    % >@@>
    ds_train=cosmo_slice(ds,train_idxs);
    ds_test=cosmo_slice(ds,test_idxs);
    % <@@<

    % compute predictions for the samples in 'ds_test' after training
    % using the samples and targets in 'ds_train'
    % >@@>
    fold_pred=cosmo_classify_lda(ds_train.samples,ds_train.sa.targets,...
                                    ds_test.samples);
    % <@@<

     % store the predictions from 'fold_pred' in the 'all_pred' vector,
    % at the positions indexed by 'test_idxs'.
    % >@@>
    all_pred(test_idxs)=fold_pred;
    % <@@<
end

% Compute classification accuracy of all_pred compared to the targets in
% the input dataset 'ds', and assign to a variable 'accuracy'
% >@@>
accuracy=mean(all_pred==ds.sa.targets);
% <@@<
fprintf(['\nLDA all categories n-fold (with partitioner): '...
            'accuracy %.3f\n'], accuracy);

% Note: cosmo_crossvalidation_measure can perform the above operations as
% well (and in an easier way), but using that function is not part of
% this exercise.

##### SOURCE END #####
--></body></html>